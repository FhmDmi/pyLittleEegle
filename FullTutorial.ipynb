{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9677d58a",
   "metadata": {},
   "source": [
    "# Full Tutorial: BCI Classification Pipeline (pyLittleEegle)\n",
    "\n",
    "This notebook presents a complete workflow for BCI data analysis, ranging from database selection to classification using Riemannian Geometry.\n",
    "\n",
    "We will use the core modules of the **Eegle-Python** package:\n",
    "1.  **`Database`**: To explore and filter the FII BCI corpus.\n",
    "2.  **`InOut`**: To load, filter, and standardize EEG recordings.\n",
    "3.  **`BCI`**: To perform covariance encoding and cross-validation.\n",
    "\n",
    "We will also leverage **`pyriemann`** and **`scikit-learn`** for the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules and package\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from pyriemann.classification import MDM, TSclassifier, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Eegle modules \n",
    "sys.path.append(os.path.abspath('src'))\n",
    "from BCI import crval, encode\n",
    "from Database import selectDB\n",
    "from InOut import readNY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae6af6",
   "metadata": {},
   "source": [
    "## 1. Database Selection and Exploration\n",
    "\n",
    "The first step is to identify relevant databases within your local corpus. Instead of loading the heavy signal data immediately, we use the `Database` module to scan the metadata.\n",
    "\n",
    "Here, we use **`selectDB`** with specific criteria:\n",
    "* **Corpus**: The path to your folder containing the NY-formatted databases.\n",
    "* **Paradigm**: `'MI'` (Motor Imagery).\n",
    "* **Classes**: We only want databases containing *at least* the classes `'feet'` and `'right_hand'`.\n",
    "\n",
    "> **Note:** Please modify the `corpusDir` variable to point to your own data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbe8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# databases selection\n",
    "corpusDir = \"C:\\\\Users\\\\doumif\\\\work\\\\OfficeWork\\\\BCI Databases\\\\NY\"\n",
    "classes = [\"feet\", \"right_hand\"]\n",
    "DBs = selectDB(corpusDir, \"MI\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543701c",
   "metadata": {},
   "source": [
    "## 2. Metadata Inspection\n",
    "\n",
    "The `selectDB` function returns a list of `infoDB` objects. These objects are lightweight and contain all necessary information to understand the structure of a database without loading the signals.\n",
    "\n",
    "By inspecting an element of this list (here `DBs[1]`), we get a formatted summary including:\n",
    "\n",
    "* **Experimental Context:** The database name, condition, and BCI paradigm (e.g., 'MI').\n",
    "* **Subject Statistics:** Total number of subjects (`nSubjects`) and the range of sessions per subject (min, max).\n",
    "* **Signal Specifications:** Sampling rate (`sr`), number of electrodes (`nSensors`), sensor names, and sensor type (e.g., 'wet', 'dry').\n",
    "* **Time parameters:** Window length (`wl`) in samples and offset.\n",
    "* **Class Balance:** A detailed statistical breakdown of **nTrials per class** (Mean ¬± Standard Deviation, Min, Max), which is crucial to check if the data is balanced.\n",
    "\n",
    "> **Note:** Additional metadata fields (DOI, hardware, software, authors, etc.) are stored in the object and can be accessed via dot notation (e.g., `DBs[1].doi` or `DBs[1].description`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c83d72",
   "metadata": {},
   "source": [
    "## 3. Single File Pipeline (Session-Specific)\n",
    "\n",
    "Before launching complex loops, it is best practice to test the pipeline on a single file (one session of one subject). Here are the three key steps:\n",
    "\n",
    "1.  **Loading (`readNY`)**:\n",
    "    * We load the `.npz` file.\n",
    "    * `classes=classes`: We keep only \"feet\" and \"right_hand\" (other classes are ignored).\n",
    "    * `bandPass=(8, 32)`: We apply a direct band-pass filter (8-32 Hz) to isolate motor rhythms (Mu/Beta bands).\n",
    "\n",
    "2.  **Encoding (`encode`)**:\n",
    "    * We transform EEG epochs into Covariance Matrices.\n",
    "    * `covtype='scm'`: We use the *Sample Covariance Matrix*. Other options include `'lwf'` (Ledoit-Wolf) or `'oas'`. See `pyriemann` for more options.\n",
    "    * The output format is automatically compatible with `pyriemann` `(n_trials, n_channels, n_channels)`.\n",
    "\n",
    "3.  **Validation (`crval`)**:\n",
    "    * We use an **MDM** (Minimum Distance to Mean) classifier, known for being robust and fast.\n",
    "    * We perform a 10-fold Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72c861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for a single file\n",
    "clf = MDM(metric='riemann', n_jobs=4)\n",
    "file = DBs[1].files[0]\n",
    "o = readNY(file, classes=classes, bandPass = (8, 32))\n",
    "ùêÇ = encode(o, paradigm = 'MI', covType='scm')\n",
    "res = crval(clf, ùêÇ, o.y, n_folds = 10, shuffle = True, random_state = 42)\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4ad95",
   "metadata": {},
   "source": [
    "## 4. Full Database Analysis\n",
    "\n",
    "Now that the pipeline is validated, we can process an entire database (all subjects/sessions).\n",
    "\n",
    "For this example, we switch the classification strategy to a method that often yields higher performance:\n",
    "* **Tangent Space (`TSclassifier`)**: Projects the covariance matrices into the Tangent Space (Euclidean).\n",
    "* **Logistic Regression**: A standard linear classifier applied to the tangent vectors, using L1 regularization (`lasso`) for feature selection.\n",
    "\n",
    "We store the average accuracy (`avgAcc`) of each file to calculate the global performance of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e087f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for a full database \n",
    "DB = DBs[1] # here BNCI2014001 will be the example\n",
    "accDB = np.zeros(len(DB.files))\n",
    "print(f\"Database name: {DB.dbName}-{DB.condition}\")\n",
    "clf = TSclassifier(clf=LogisticRegression(penalty='l1', solver='saga', max_iter=1000, n_jobs=4))\n",
    "for f, file in enumerate(DB.files):\n",
    "    nf = len(DB.files)\n",
    "    print(f\"file {f+1} of {nf}\")\n",
    "    o = readNY(file, classes=classes, bandPass = (8, 32))\n",
    "    ùêÇ = encode(o, paradigm = 'MI', covType='scm')\n",
    "    res = crval(clf, ùêÇ, o.y, n_folds = 10, shuffle = True, random_state = 89)\n",
    "    accDB[f] = res.avgAcc\n",
    "\n",
    "print(f\"\\nAverage accuracy for {DB.dbName}-{DB.condition} : \", np.round(np.mean(accDB)*100, decimals = 2), \"% ¬±\", np.round(np.std(accDB)*100, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6636f33",
   "metadata": {},
   "source": [
    "## 5. Large-Scale Benchmark (Multi-Database)\n",
    "\n",
    "Finally, we can launch an evaluation across all databases selected by `selectDB`.\n",
    "\n",
    "* We iterate through every database in `DBs`.\n",
    "* Inside, we iterate through every file.\n",
    "* We use a **LinearSVC** (Support Vector Machine) in the Tangent Space.\n",
    "* The results (accuracy per file) are saved into text files (`.txt`) named after the database for further statistical analysis.\n",
    "\n",
    "> **Warning:** Depending on the size of your data and CPU cores, this cell might take some time to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for all database corresponding to arg classes\n",
    "accDBs = np.zeros(len(DBs))\n",
    "clf = TSclassifier(clf=LinearSVC(max_iter=1000))\n",
    "\n",
    "for db, DB in enumerate(DBs):\n",
    "    accDB = np.zeros(len(DB.files))\n",
    "    print(f\"Database name: {DB.dbName}-{DB.condition}\")\n",
    "    for f, file in enumerate(DB.files):\n",
    "        nf = len(DB.files)\n",
    "        print(f\"file {f+1} of {nf}\")\n",
    "        o = readNY(file, classes=classes, bandPass = (8, 32))\n",
    "        ùêÇ = encode(o, paradigm = 'MI', covType='scm')\n",
    "        res = crval(clf, ùêÇ, o.y, n_folds = 10, shuffle = True, random_state = 89)\n",
    "        accDB[f] = res.avgAcc\n",
    "    print(f\"\\nAverage accuracy for {DB.dbName}-{DB.condition} : \", np.round(np.mean(accDB)*100, decimals = 2), \"% ¬±\", np.round(np.std(accDB)*100, decimals=2))\n",
    "    np.savetxt(f\"acc_{DB.dbName}-{DB.condition}.txt\", accDB, fmt=\"%.5f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
